# **🗂️ [프로젝트 아카이브] 설정 기반 동적 크롤러 시스템**

- **프로젝트명**: 확장 가능한 상품 정보 크롤러 서비스
- **최종 수정일**: 2025년 9월 16일
- **핵심 기술**: Node.js, Playwright, BullMQ, Fastify, Supabase
- **문서 목적**: 프로젝트의 최종 아키텍처, 핵심 설계 원칙, 주요 산출물을 정리하여 신규 참여자나 미래의 내가 빠르게 이해할 수 있도록 보관

---

## **1. 프로젝트 개요 (Project Overview)** 📝

본 프로젝트는 여러 온라인 쇼핑몰의 상품 정보(가격, 재고 등)를 자동으로 수집하여, 사용자에게 가격 비교 및 상품 추천 서비스를 제공하는 것을 목표로 합니다. 이를 위해 **유지보수와 확장이 용이**하고 **안정적**이며 **보안**이 고려된 동적 크롤러 시스템을 구축합니다.

---

## **2. 최종 아키텍처 (Final Architecture)** 🏗️

### **시스템 구성도**

```
+----------------+      +---------------------+      +-----------------+
|   Scheduler    |----->| (Authenticated)     |      |      User       |
| (node-cron)    |      | API Server (Fastify)|<-----| (Web/App Client)|
+----------------+      | + Rate Limiting     |      +-----------------+
                        |                     |             ^  | Realtime
                        +----------+----------+             |  | Subscription
                                   |                        |  V
                          (Add Job) V                       |
                        +-----------------------------------+--+
                        |           Supabase Platform          |
                        |      (DB w/ RLS, Storage, etc.)      |
                        +------------------+-------------------+
                                           ^ (Save Data)
+------------------------------------------|-------------------------+
| Restricted Network Environment (e.g., VPC)                        |
|                                                                   |
|         +--------------------------------|------------+           |
|         |         Crawler Workers (Playwright)        |           |
|         +---------------------------------------------+           |
|                              ^ (Process Job)                      |
|                       +------|------------+                       |
|                       |  Job Queue (BullMQ) |                       |
|                       +-------------------+                       |
+-------------------------------------------------------------------+
```

### **최종 기술 스택**

- **언어/플랫폼**: TypeScript, Node.js
- **API 서버**: Fastify
- **크롤링 엔진**: Playwright
- **작업 큐**: BullMQ (Redis 기반)
- **백엔드 플랫폼 (BaaS)**: Supabase
- **배포/운영**: Docker, Kubernetes/Cloud Run

---

## **3. 핵심 설계 원칙 (Core Design Principles)** 💡

### **Principle 1: 설정 기반 동적 크롤링 (Configuration-Driven)**

- **설명**: 크롤러의 핵심 로직(엔진)과 각 사이트를 크롤링하는 방법(설정)을 완벽하게 분리합니다.
- **효과**: 신규 사이트 추가나 기존 사이트의 구조 변경 시, **코드 수정 없이 DB에 저장된 설정(JSON)만 수정**하면 되므로 확장성과 유지보수성이 극대화됩니다.

### **Principle 2: 작업 큐를 통한 확장성 및 안정성 확보 (Scalable & Reliable)**

- **설명**: API 서버와 크롤러 워커 사이에 BullMQ라는 작업 큐를 두어 요청과 실제 처리를 분리합니다.
- **효과**: 갑작스러운 요청 증가에도 시스템이 안정적으로 유지되며(부하 제어), 크롤링 처리량이 부족할 경우 **워커(Worker)의 수만 늘리면** 간단하게 성능을 확장할 수 있습니다.

### **Principle 3: 보안 우선 설계 (Security-First)**

- **설명**: API, 데이터베이스, 인프라 등 모든 계층에서 보안을 고려합니다.
- **효과**:
  - **API**: Rate Limiting, API Key 인증으로 DoS 및 무단 사용 방지
  - **데이터베이스**: Supabase의 RLS(행 수준 보안)로 데이터 접근 권한 최소화
  - **인프라**: Secrets Manager를 통한 키 관리, 네트워크 격리를 통한 SSRF 방지

---

## **4. 주요 프로세스 및 산출물 (Key Processes & Deliverables)** 📂

본 프로젝트를 진행하며 다음과 같은 핵심 가이드와 문서가 작성되었습니다.

- **[서비스 설계서 최종본]**

  - **목적**: 프로젝트의 전체 아키텍처, 기술 스택, 컴포넌트별 역할, 데이터 모델, 보안 정책 등을 총망라한 종합 설계 문서.

- **[작업 가이드 프롬프트 최종본]**

  - **목적**: 설계서를 기반으로 실제 개발을 진행할 수 있도록, 환경 설정부터 기능 구현, 보안 적용, 배포까지의 과정을 단계별로 안내하는 Vibe Coding 가이드.

- **[신규 타겟 분석 가이드 및 구현 예시]**

  - **목적**: 새로운 크롤링 대상 사이트를 추가할 때 필요한 분석 항목(CSS 선택자, 페이지네이션 전략 등)을 체계적으로 정리하고, 그 결과를 실제 동적 크롤러 코드에서 어떻게 활용하는지 보여주는 실용적인 가이드.

---

## **이후 진행**

- **최종 수정일**: 2025년 9월 16일
- **문서 목적**: 아카이브 내용 이후 진행할 사항을 추가적으로 정리한 부분.

### **1. ⚙️ 크롤러 고도화 (Advanced Crawler Enhancements)**

현재의 크롤러는 기본적인 동작에 충실합니다. 실제 운영 환경에서는 다양한 예외 상황에 대응할 수 있도록 아래 기능들을 추가하는 것이 좋습니다.

- **고급 에러 처리 및 재시도(Retry) 로직**:

  - **상황 정의**: 특정 사이트의 CSS 선택자가 변경되었을 때, 사이트가 일시적으로 다운되었을 때, CAPTCHA(자동입력방지)가 나타났을 때 등 실패 유형을 정의합니다.
  - **자동화**: BullMQ의 재시도 기능을 활용하여, 일시적인 오류(예: 사이트 다운)는 몇 분 뒤 자동으로 재시도하도록 설정하고, 영구적인 오류(예: 선택자 변경)는 실패로 기록하고 개발자에게 알림을 보내도록 구현합니다.

- **차단(Block) 회피 기술 도입**:

  - **프록시 로테이션 (Proxy Rotation)**: 대규모 크롤링 시 동일 IP에서의 반복적인 접근은 차단될 확률이 높습니다. 유료 프록시 서비스를 연동하여 요청마다 IP를 변경해주는 기능을 추가합니다.
  - **브라우저 핑거프린트(Fingerprint) 관리**: 일부 고도화된 사이트는 단순 User-Agent 외에 브라우저의 다양한 속성(화면 해상도, 설치된 폰트 등)을 체크하여 봇을 탐지합니다. Playwright의 기능을 활용하여 이러한 값들을 실제 사용자처럼 보이도록 다양화하는 것을 고려할 수 있습니다.

- **데이터 변경 감지 로직**:
  - **효율성 증대**: 매번 모든 데이터를 덮어쓰기(UPDATE)하는 대신, 크롤링한 데이터(예: 가격)가 DB에 저장된 최신 데이터와 **실제로 달라졌을 경우에만** `prices` 테이블에 새로 추가(INSERT)하도록 로직을 개선합니다. 이는 불필요한 DB 쓰기를 줄여 성능을 향상시키고, 가격 변경 이력을 더 명확하게 관리할 수 있게 합니다.

---

### **2. 🖥️ 프론트엔드 및 사용자 기능 구현 (Frontend & User Features)**

크롤러가 수집한 데이터를 실제로 가치 있게 만드는 단계입니다.

- **검색 및 필터링 기능**:

  - 수집된 상품들을 사용자가 원하는 조건(브랜드, 가격대, 상품명 등)으로 검색하고 필터링할 수 있는 UI를 개발합니다. Supabase의 PostgreSQL은 이러한 복합적인 쿼리에 강점을 가집니다.

- **사용자 알림 기능**:
  - **가격 변동 알림**: 사용자가 특정 상품을 '관심 상품'으로 등록하면, 해당 상품의 가격이 떨어졌을 때 이메일이나 앱 푸시 알림을 보내주는 기능을 구현합니다. 이는 Supabase의 Realtime 기능과 서버리스 함수(Edge Functions)를 결합하면 효과적으로 만들 수 있습니다.

---

### **3. 🚀 배포 및 운영 자동화 (DevOps & Automation)**

수동으로 관리하던 부분들을 자동화하여 운영 효율을 극대화합니다.

- **CI/CD 파이프라인 구축**:

  - GitHub Actions 같은 도구를 사용하여, 코드가 Git에 푸시될 때마다 자동으로 테스트, 빌드, Docker 이미지 생성, 클라우드 환경(Cloud Run 등)에 배포되는 파이프라인을 구축합니다.

- **고급 모니터링 및 알림 시스템**:
  - **대시보드 구축**: Grafana, Datadog 같은 전문 모니터링 도구를 사용하여 작업 큐의 상태, 사이트별 크롤링 성공률, 워커의 리소스 사용량 등을 한눈에 볼 수 있는 대시보드를 만듭니다.
  - **이상 징후 알림**: "지난 1시간 동안 A 사이트의 크롤링 실패율이 50% 이상"과 같은 특정 조건이 충족되면 Slack이나 이메일로 즉시 알림을 보내는 시스템을 구축합니다.

---

### **4. ⚖️ 관리 및 정책 (Management & Policy)**

기술 외적으로 프로젝트의 장기적인 안정성을 위해 고려해야 할 부분입니다.

- **단계적 확장 계획 (Phased Rollout)**:

  - 처음부터 수십 개의 사이트를 대상으로 하기보다, 구조가 비교적 간단한 2~3개의 사이트를 먼저 안정적으로 운영하며 시스템을 검증합니다. 이후 운영 노하우를 바탕으로 점차 대상 사이트를 늘려나가는 것이 안전합니다.

- **크롤링 정책 검토**:
  - 각 대상 사이트의 `robots.txt` 파일을 확인하여 크롤링을 명시적으로 금지하는 페이지는 수집 대상에서 제외합니다.
  - 사이트의 이용약관(Terms of Service)을 검토하여 무단 수집 및 재가공에 대한 법적 문제는 없는지 확인하는 것이 장기적으로 안전한 서비스 운영에 도움이 됩니다.
